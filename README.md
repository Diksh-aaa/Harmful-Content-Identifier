# Harmful-Content-Identifier

AIM: Develop a model that can identify nude/vulgar/gore/violent images.

-----------------------

DATASETS USED: 

*x1101/nsfw-full, DarkyMan/nsfw-image-classification* for nude and pornographic images

*NeuralShell/Gore-Blood-Dataset-v1.0* for gore images

*rtb1271/human_fight_detection* for Violent images

*Alternatively if you have enough computing resources you can use **deepghs/nsfw_detect** to train your model for nudity/pornographic images/vulgarity.*

----------------------

PURPOSE: One major reason is to identify harmful and not safe to work (NSFW) content from websites to avoid harrassment online, or to make web safer for use for general public. 
